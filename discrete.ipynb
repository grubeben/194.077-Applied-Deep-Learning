{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the control room for A2C training\n",
    "It will allow you to..\n",
    "* Play around with an n-step A2C agent (**I**), follow it's live performance and visualize the learned policy (**II**)\n",
    "* Get an insight into how different agent configurations affect performance and convergence stability (**III** )\n",
    "\n",
    "### How to\n",
    "* If you want to see the agent training in live just follow the notebook instructions in the order presented\n",
    "* If you are in a hurry and are just interested in the results this A2C- implementation achieved, pfast forward to (**III**)\n",
    "* For details on the methods that are used to train the agent, please refer to the 'train_discrete.py' and 'a2cagent_discrete.py' scripts, you will find documentation there\n",
    "* For more theoretical background on the A2C method, please refer to the **FOUNDATIONS** section in the README.md file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "#hide loading information\n",
    "\n",
    "#import python files and modules\n",
    "import tensorflow as tf\n",
    "import datetime, os\n",
    "import a2cagent_discrete\n",
    "import train_discrete as train\n",
    ";"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I Use the A2C to learn a control task\n",
    "#### Enable TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Training Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "session = train.Session(converged_reward_limit=195, specification=\"TEST_RUN\", activation_function='mish', initializer='normal', state_normalization=False,\n",
    "          batch_normalization=False, use_existing_policy=False)\n",
    "# run for 1 batch to initialize\n",
    "session.train(max_num_batches=1)\n",
    "# define tensorflow data log\n",
    "tb_dir = session.model.model_path + '/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start TensorBoard Session\n",
    "1. Run the next block twice (until TensorBoard shows up)\n",
    "2. Use the 'Settings'-button on the top right corner and activate 'Reload data'  (alternatively make yourself familiar with the 'Update' button)\n",
    "\n",
    "##### *NOTE (y-axis)*: rewards are visualized over 'number of episodes', while losses are visualized over 'number of batches'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir {tb_dir}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start training\n",
    "1. By executing the next block, training will be initialized\n",
    "2. In order to watch progress return to TensorBoard Dashboard\n",
    "\n",
    "Dashboard will be updated every 30sec or else when you hit the 'Update' button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "\"\"\"Training will run either till convergence (defined by 'converged_reward_limit' during Session(__init__)) or until 'max_num_batches' is surpassed\"\"\"\n",
    "session.train(max_num_batches=1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II Test and Visualisation Section\n",
    "### Let's visualize the the policy learned above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "from tkinter import Tk\n",
    "from tkinter.filedialog import askopenfilename\n",
    "\n",
    "import gym\n",
    "import pygame\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from IPython import display\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Note:* \n",
    "* train() method can be interrupted at any point in time, the best policy (~ the one that led to the highest rewarded episode) will still be saved! \n",
    "This might be interesting in order to see what the control system is able to do after a certain number of episodes\n",
    "* If train() method is called on the same Session() object again, it will restart training from that policy!\n",
    "\n",
    "### Visualize policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.test(num_episodes=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III Convergence analysis for different agent configurations\n",
    "\n",
    "The section above enables us to play around with the A2C-agent and get a feeling for what it is capable of doing. However, as proposed in the goals of this project, I am to set up an agent for continuous action spaces, which should not be so much more challenging altogether, but has up-to-date posed quite a hurdle to me (as demonstrated in the continuous.ipynb)\n",
    "\n",
    "Anyways, while searching for solutions to get the continuous agent running (browsing through forums, documentation and papers), I discovered that there is a lot to be gained if one experiments with some tweaks to the agent architecture. I wanted to see which of the proposed adaptions would make the discrete action-space agent more effective in order to find out what might help me to solve the continuous action-space agent later!\n",
    "\n",
    "This chapter is the result of this investigation and will show how much of an impact different **weight-initializers**, **activation-functions** and **normalization-layers** have on training performance.  \n",
    "\n",
    "### On the adaptions\n",
    "* **Weight-initializers**: If we manage to initialize the NN-weights in a fortunate manner right from the beginning, training might progress faster\n",
    "* **Normalization-layers**: \n",
    "    * **Normalizing the input to the NN** (based on prior state experiences) the network will act solely on the variance from what it has known to be the average. This might decrease sensibility to inputs with large variance (obsversation space [-inf, inf])\n",
    "    * **Batch Normalization** Basically does the same in between layers of the NN\n",
    "* **Activation functions**: This is interesting, because different activation functions obviously return differently fine-grained information of what the input the the layer has been, however, some of them are more costly to compute than others, hence: the ideal activation has to be a compromise of both\n",
    "\n",
    "\n",
    "### Experiment\n",
    "In order to demonstrate the differences in training stability and efficiency I trained differently configured agents (until they were capable of solving the 'CartPole-v1'-problem **OR have had 1000 batches = 64 000 steps to train**). \n",
    "\n",
    "The resulting policies are safed in the *'training_discrete/pretrained'* folder and there convergence behaviour is visualized below\n",
    "\n",
    "**Please note, that hyperparameters such as *learning rate*, *batch_size*, *layer_size* etc. have not been altered.**\n",
    "\n",
    "### Notes regarind TensorBoard\n",
    "* It might again be necessary to run the block twice\n",
    "* Untick the failing agents on the left bar in order to 'unsqueeze' the perfromance of those that acutally delivered fast learning results OR BETTER: choose **Settings> General> Horizontal Axis> Relative** on the right side of the DashBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_pretrained = os.getcwd()+'/training_discrete/pretrained/'\n",
    "\n",
    "%tensorboard --logdir {tb_pretrained}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some further information and analysis on the graphs above\n",
    "\n",
    "1. **relue_normal_no_normalization**: did not meet convergence criteria\n",
    "2. **relue_normal_state_normalization**: CONVERGED in  379  batches /  24256  steps\n",
    "2. **relue_normal_state&batch_normalization**: did not meet convergence criteria / collapsed\n",
    "2. **relue_normal_batch_normalization**: did not meet convergence criteria / collapsed\n",
    "2. **relue_xavier_no_normalization**: CONVERGED in  427  batches /  27328  steps\n",
    "2. **mish_xavier_no_normalization**: CONVERGED in  513  batches /  32832  steps\n",
    "2. **mish_xavier_no_normalization**: CONVERGED in  333  batches /  21312  steps\n",
    "### 8. **mish_xavier_state_normalization**: CONVERGED in  330  batches /  21120  steps\n",
    "\n",
    "## Learnings\n",
    "\n",
    "* **batch_normalization** does not work great here..\n",
    "* **state_normalization** does ;)\n",
    "* **xavier_initialization** seems to smoothen the learning process\n",
    "* **mish_activation** is a hero\n",
    "\n",
    "* **low total_loss** (=combined criti, actor and entropy loss over one batch) is not necessarily the best indicator for a good policy\n",
    "\n",
    "## Finally the best agent's policy demonstrated for 1 episode\n",
    "### ... chances are: this is a loooong episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_session = train.Session(converged_reward_limit=195, specification=\"TEST_RUN\", activation_function='mish', initializer='xavier', state_normalization=True,\n",
    "          batch_normalization=False, use_existing_policy=True, policy='pretrained/CartPole-v1_mish_normal_STATE_NORMALIZATION/')\n",
    "best_session.test(num_episodes=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proof that the agent can also deal with different gym-problems:\n",
    "**Note: The 'MountainCar-0' is not directly compatible because it's action space maps onto $[-inf,0,inf]$ whereas our agent would return $[0,1,2]$**\n",
    "\n",
    "**However, 'Acrobat-v1' does work since the action options map onto $[0,1,2]$ (btw. this environment has a 6-D observation_space, so that's more data to work with than 4D-'CartPole-v1')**\n",
    "\n",
    "**(This environment is a 2-link robot that wants to lift it's center of mass as high as possible)**\n",
    "\n",
    "**As always: Run block twice!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_acrobat = os.getcwd()+'/training_discrete/Acrobot-v1_mish_normal_PROOF/'\n",
    "\n",
    "%tensorboard --logdir {tb_acrobat}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acrobat_session = train.Session(converged_reward_limit=-100, env_str=\"Acrobot-v1\", specification=\"PROOF\", activation_function='mish', initializer='normal', state_normalization=False,\n",
    "#                     batch_normalization=False, use_existing_policy=False)\n",
    "# acrobat_session.train(max_num_batches=500)\n",
    "\n",
    "acrobat_session = train.Session(converged_reward_limit=-100, env_str=\"Acrobot-v1\", specification=\"PROOF\", activation_function='mish', initializer='normal', state_normalization=False,\n",
    "                  batch_normalization=False, use_existing_policy=True, policy='Acrobot-v1_mish_normal_PROOF/')\n",
    "acrobat_session.test(num_episodes=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THANKS FOR CHECKING THIS OUT!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
